{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Allow GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Initialize GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Rutas a las carpetas de entrenamiento, prueba y validación para imágenes y máscaras\n",
    "train_frames_dir = '../Frames/TRAIN'\n",
    "train_Landmarks_dir = './Landmarks/TRAIN'\n",
    "test_frames_dir = '../Frames/TEST'\n",
    "test_Landmarks_dir = './Landmarks/TEST'\n",
    "val_frames_dir = '../Frames/VAL'\n",
    "val_Landmarks_dir = './Landmarks/VAL'\n",
    "\n",
    "# Funciones para cargar y preprocesar imagen y máscara y convertirlo a solo un canal\n",
    "def load_image(file_path):\n",
    "    image = cv2.imread(file_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def load_landmarks(file_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(file_path):\n",
    "        image=cv2.imread(os.path.join(file_path, filename))\n",
    "        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        images.append(grayscale_image)\n",
    "\n",
    "    stacked_image = np.stack(images, axis=-1)  # axis=-1 indica la dimensión de los canales\n",
    "\n",
    "    return stacked_image\n",
    "\n",
    "train_image_files = [os.path.join(train_frames_dir, filename) for filename in os.listdir(train_frames_dir)]\n",
    "train_Landmarks_files = [os.path.join(train_Landmarks_dir, filename) for filename in os.listdir(train_Landmarks_dir)]\n",
    "test_image_files = [os.path.join(test_frames_dir, filename) for filename in os.listdir(test_frames_dir)]\n",
    "test_Landmarks_files = [os.path.join(test_Landmarks_dir, filename) for filename in os.listdir(test_Landmarks_dir)]\n",
    "val_image_files = [os.path.join(val_frames_dir, filename) for filename in os.listdir(val_frames_dir)]\n",
    "val_Landmarks_files = [os.path.join(val_Landmarks_dir, filename) for filename in os.listdir(val_Landmarks_dir)]\n",
    "\n",
    "train_images = [load_image(file) for file in train_image_files]\n",
    "train_Landmarks = [load_landmarks(file) for file in train_Landmarks_files]\n",
    "\n",
    "test_images = [load_image(file) for file in test_image_files]\n",
    "test_Landmarks = [load_landmarks(file) for file in test_Landmarks_files]\n",
    "\n",
    "val_images = [load_image(file) for file in val_image_files]\n",
    "val_Landmarks = [load_landmarks(file) for file in val_Landmarks_files]\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_Landmarks = np.array(train_Landmarks)\n",
    "test_images = np.array(test_images)\n",
    "test_Landmarks = np.array(test_Landmarks)\n",
    "val_images = np.array(val_images)\n",
    "val_Landmarks = np.array(val_Landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape:  (14920, 112, 112)\n",
      "Train Landmarks shape:  (14920, 112, 112, 7)\n",
      "Test images shape:  (2552, 112, 112)\n",
      "Test Landmarks shape:  (2552, 112, 112, 7)\n",
      "Validation images shape:  (2574, 112, 112)\n",
      "Validation Landmarks shape:  (2574, 112, 112, 7)\n"
     ]
    }
   ],
   "source": [
    "# Contar que todas las imágenes y máscaras tengan el mismo tamaño\n",
    "print(\"Train images shape: \", train_images.shape)\n",
    "print(\"Train Landmarks shape: \", train_Landmarks.shape)\n",
    "print(\"Test images shape: \", test_images.shape)\n",
    "print(\"Test Landmarks shape: \", test_Landmarks.shape)\n",
    "print(\"Validation images shape: \", val_images.shape)\n",
    "print(\"Validation Landmarks shape: \", val_Landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agremos el path de la carpeta de modelos para poder importar el modelo\n",
    "import sys\n",
    "sys.path.append(r'R:\\Codes\\Reto\\Modelos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, BatchNormalization, Activation, ReLU, Conv2DTranspose\n",
    "\n",
    "# Definición de la arquitectura U-Net\n",
    "def unet_model(input_shape=(112, 112, 1), n_classes=1, kernel_out=1, activation='sigmoid'):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    conv1 = Conv2D(64, 3, padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = ReLU()(conv1)\n",
    "    conv1 = Conv2D(64, 3, padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = ReLU()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = ReLU()(conv2)\n",
    "    conv2 = Conv2D(128, 3, padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = ReLU()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = ReLU()(conv3)\n",
    "    conv3 = Conv2D(256, 3, padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = ReLU()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, 3, padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = ReLU()(conv4)\n",
    "    conv4 = Conv2D(512, 3, padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = ReLU()(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = ReLU()(conv5)\n",
    "    conv5 = Conv2D(1024, 3, padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = ReLU()(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(drop5)\n",
    "    up6 = concatenate([up6, drop4])\n",
    "    conv6 = Conv2D(512, 3, padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = ReLU()(conv6)\n",
    "    conv6 = Conv2D(512, 3, padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = ReLU()(conv6)\n",
    "\n",
    "    up7 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n",
    "    up7 = concatenate([up7, conv3])\n",
    "    conv7 = Conv2D(256, 3, padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = ReLU()(conv7)\n",
    "    conv7 = Conv2D(256, 3, padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = ReLU()(conv7)\n",
    "\n",
    "    up8 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n",
    "    up8 = concatenate([up8, conv2])\n",
    "    conv8 = Conv2D(128, 3, padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = ReLU()(conv8)\n",
    "    conv8 = Conv2D(128, 3, padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = ReLU()(conv8)\n",
    "\n",
    "    up9 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n",
    "    up9 = concatenate([up9, conv1])\n",
    "    conv9 = Conv2D(64, 3, padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = ReLU()(conv9)\n",
    "    conv9 = Conv2D(64, 3, padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = ReLU()(conv9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, kernel_out, activation=activation)(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10, name='U-Net_2')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = unet_model(input_shape=(112, 112, 1), n_classes=7, kernel_out=1, activation='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# with tf.device('CPU'):\n",
    "#     train_dataset = tf.data.Dataset.from_tensor_slices((train_images[:700], train_Landmarks[:700]))\n",
    "#     train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "#     val_dataset = tf.data.Dataset.from_tensor_slices((val_images[:300], val_Landmarks[:300]))\n",
    "#     val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "#     test_dataset = tf.data.Dataset.from_tensor_slices((test_images[:300], test_Landmarks[:300]))\n",
    "#     test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "with tf.device('CPU'):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_Landmarks))\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_Landmarks))\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_Landmarks))\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_flat = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_flat = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_flat * y_pred_flat)\n",
    "    return (2.0 * intersection + smooth) / (tf.keras.backend.sum(y_true_flat) + tf.keras.backend.sum(y_pred_flat) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(), loss=dice_loss, metrics=[dice_coefficient, 'accuracy'])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933/933 [==============================] - 190s 203ms/step - loss: 4.9523 - accuracy: 0.1688 - val_loss: 4.9607 - val_accuracy: 0.2208\n",
      "Epoch 2/100\n",
      "933/933 [==============================] - 189s 203ms/step - loss: 4.9289 - accuracy: 0.1670 - val_loss: 4.9791 - val_accuracy: 0.1239\n",
      "Epoch 3/100\n",
      "933/933 [==============================] - 190s 203ms/step - loss: 4.9102 - accuracy: 0.1697 - val_loss: 4.9651 - val_accuracy: 0.0768\n",
      "Epoch 4/100\n",
      "933/933 [==============================] - 189s 203ms/step - loss: 4.8911 - accuracy: 0.1788 - val_loss: 4.9550 - val_accuracy: 0.0774\n",
      "Epoch 5/100\n",
      "933/933 [==============================] - 183s 196ms/step - loss: 4.8714 - accuracy: 0.1775 - val_loss: 4.9628 - val_accuracy: 0.0955\n",
      "Epoch 6/100\n",
      "933/933 [==============================] - 158s 169ms/step - loss: 4.8512 - accuracy: 0.1768 - val_loss: 4.9830 - val_accuracy: 0.1430\n",
      "Epoch 7/100\n",
      "933/933 [==============================] - 158s 169ms/step - loss: 4.8251 - accuracy: 0.1688 - val_loss: 4.9977 - val_accuracy: 0.0499\n",
      "Epoch 8/100\n",
      "933/933 [==============================] - 173s 185ms/step - loss: 4.7994 - accuracy: 0.1630 - val_loss: 5.0155 - val_accuracy: 0.1109\n",
      "Epoch 9/100\n",
      "933/933 [==============================] - 169s 181ms/step - loss: 4.7745 - accuracy: 0.1679 - val_loss: 5.0335 - val_accuracy: 0.0738\n"
     ]
    }
   ],
   "source": [
    "# Agregamos un callback para guardar el modelo cada 5 épocas y otro para detener el entrenamiento si no hay mejora en 10 épocas\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 10s 63ms/step - loss: 5.0318 - accuracy: 0.0737\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model history information to a json file\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a folder name\n",
    "name_folder = 'landmark_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "path = '../Pruebas/{}/'.format(name_folder)\n",
    "os.mkdir(path)\n",
    "\n",
    "# Save history to a json file\n",
    "path_json = '{}history.json'.format(path)\n",
    "\n",
    "with open(path_json, 'w') as fp:\n",
    "    json.dump(history.history, fp)\n",
    "\n",
    "# Save model\n",
    "path_model = '{}model.h5'.format(path)\n",
    "model.save(path_model)\n",
    "\n",
    "# Save model summary to a txt file\n",
    "path_summary = '{}summary.txt'.format(path)\n",
    "with open(path_summary, 'w') as fp:\n",
    "    model.summary(print_fn=lambda x: fp.write(x + '\\n'))\n",
    "\n",
    "# Save model metrics to a txt file\n",
    "path_metrics = '{}metrics.txt'.format(path)\n",
    "with open(path_metrics, 'w') as fp:\n",
    "    fp.write('Test loss: {}\\n'.format(evaluation[0]))\n",
    "    fp.write('Test accuracy: {}\\n'.format(evaluation[2]))\n",
    "    fp.write('Model optimizer: {}\\n'.format(model.optimizer.__class__.__name__))\n",
    "    fp.write('Model activation: {}\\n'.format(model.layers[-1].activation.__name__))\n",
    "    fp.write('Model learning rate: {}\\n'.format(model.optimizer.lr.numpy()))\n",
    "\n",
    "# Generate a folder inside the folder of the model for the images\n",
    "path_images = '{}images/'.format(path)\n",
    "os.mkdir(path_images)\n",
    "\n",
    "# Generate a folder inside the folder of the images for plot images\n",
    "path_plot_images = '{}plot_images/'.format(path_images)\n",
    "os.mkdir(path_plot_images)\n",
    "\n",
    "# Generate a folder inside the folder of the images for prediction images\n",
    "path_test_images = '{}predicitons/'.format(path_images)\n",
    "os.mkdir(path_test_images)\n",
    "\n",
    "# Save prediction images\n",
    "# Función para visualizar una muestra de imágenes y sus máscaras de segmentación predichas\n",
    "def visualize_landmarks(images, landmarks, predictions):\n",
    "    num_images = len(images)\n",
    "\n",
    "    for i in range(num_images):  # Renamed the inner loop variable to 'j'\n",
    "        # Set up the subplots\n",
    "        image = images[i].reshape((112, 112))\n",
    "        prediction = predictions[i].reshape((112, 112, 7)) * 255.0\n",
    "        landmark = landmarks[i] * 255.0  # No need to reshape\n",
    "        fig, axs = plt.subplots(2, 9, figsize=(15, 5))\n",
    "\n",
    "        # Display the test image spanning the first two columns of both rows\n",
    "        axs[0, 0].imshow(image, cmap='grey')\n",
    "        axs[0, 0].axis('off')\n",
    "        axs[0, 0].set_title('Test Image') \n",
    "        axs[1, 0].axis('off')\n",
    "\n",
    "        # Combine all channels of prediction_landmarks by taking the mean\n",
    "        combined_prediction = np.mean(prediction, axis=-1)\n",
    "\n",
    "        # CObine all channels of ground_truth_landmarks by taking the mean\n",
    "        combined_ground_truth = np.mean(landmark, axis=-1)\n",
    "\n",
    "        # Display the predicted and ground truth landmarks in the first column of the second row all in one image\n",
    "        axs[0, 1].imshow(combined_prediction, cmap='grey')\n",
    "        axs[0, 1].set_title('Predicted')\n",
    "        axs[0, 1].axis('off')\n",
    "        axs[1, 1].imshow(combined_ground_truth, cmap='grey')\n",
    "        axs[1, 1].set_title('Ground Truth')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "        # Display the predicted landmarks in the first row\n",
    "        for k in range(7):  # Renamed the loop variable to 'k'\n",
    "            axs[0, k+2].imshow(prediction[ :, :, k], cmap='grey')\n",
    "            axs[0, k+2].axis('off')\n",
    "            axs[0, k+2].set_title('Predicted ' + str(k + 1))\n",
    "\n",
    "        # Display the ground truth landmarks in the second row\n",
    "        for k in range(7):  # Renamed the loop variable to 'k'\n",
    "            axs[1, k+2].imshow(landmark[ :, :, k], cmap='grey')\n",
    "            axs[1, k+2].axis('off')\n",
    "            axs[1, k+2].set_title('Ground Truth ' + str(k + 1))\n",
    "\n",
    "        plt.savefig('{}{}.png'.format(path_test_images, i))\n",
    "\n",
    "\n",
    "# Supongamos que tienes un conjunto de datos de prueba con imágenes y máscaras\n",
    "# Puedes utilizar el modelo para obtener las máscaras predichas en el conjunto de prueba\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Elije algunas muestras aleatorias para visualizar\n",
    "num_samples_to_visualize = 5\n",
    "sample_indices = np.random.choice(len(test_images[:10]), num_samples_to_visualize, replace=False)\n",
    "\n",
    "sample_images = [test_images[i] for i in sample_indices]\n",
    "sample_landmarks = [test_Landmarks[i] for i in sample_indices]\n",
    "sample_predictions = [predictions[i] for i in sample_indices]\n",
    "\n",
    "# Llama a la función para visualizar las imágenes y máscaras\n",
    "visualize_landmarks(sample_images, sample_landmarks, sample_predictions)\n",
    "\n",
    "# Save plot images\n",
    "for i in ['accuracy', 'loss']:\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history['val_{}'.format(i)])\n",
    "    plt.title('Model {}'.format(i))\n",
    "    plt.ylabel(i)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig('{}{}.png'.format(path_plot_images, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
