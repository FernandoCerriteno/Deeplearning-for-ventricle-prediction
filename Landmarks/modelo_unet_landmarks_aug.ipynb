{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Allow GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Initialize GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.transform import rotate\n",
    "\n",
    "# Rutas a las carpetas de entrenamiento, prueba y validación para imágenes y máscaras\n",
    "train_frames_dir = '../Frames/TRAIN'\n",
    "train_Landmarks_dir = './Landmarks/TRAIN'\n",
    "test_frames_dir = '../Frames/TEST'\n",
    "test_Landmarks_dir = './Landmarks/TEST'\n",
    "val_frames_dir = '../Frames/VAL'\n",
    "val_Landmarks_dir = './Landmarks/VAL'\n",
    "\n",
    "# Funciones para cargar y preprocesar imagen y máscara y convertirlo a solo un canal\n",
    "def load_image(file_path):\n",
    "    image = cv2.imread(file_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def load_landmarks(file_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(file_path):\n",
    "        image=cv2.imread(os.path.join(file_path, filename))\n",
    "        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        images.append(grayscale_image)\n",
    "\n",
    "    stacked_image = np.stack(images, axis=-1)  # axis=-1 indica la dimensión de los canales\n",
    "    stacked_image = stacked_image / 255.0\n",
    "    return stacked_image\n",
    "\n",
    "# Data augmentation: Rotations and Flips\n",
    "def augment_data(images, landmarks):\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "    for image, mask in zip(images, landmarks):\n",
    "        augmented_images.append(image)\n",
    "        augmented_masks.append(landmarks)\n",
    "        # # 20 degree rotations\n",
    "        # augmented_images.append(rotate(image, angle=20, reshape=False))\n",
    "        # augmented_masks.append(rotate(landmarks, angle=20, reshape=False))\n",
    "        # augmented_images.append(rotate(image, angle=-20, reshape=False))\n",
    "        # augmented_masks.append(rotate(landmarks, angle=-20, reshape=False))\n",
    "\n",
    "        # Flips\n",
    "        augmented_images.append(np.fliplr(image))\n",
    "        augmented_masks.append(np.fliplr(landmarks))\n",
    "\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "train_image_files = [os.path.join(train_frames_dir, filename) for filename in os.listdir(train_frames_dir)]\n",
    "train_Landmarks_files = [os.path.join(train_Landmarks_dir, filename) for filename in os.listdir(train_Landmarks_dir)]\n",
    "test_image_files = [os.path.join(test_frames_dir, filename) for filename in os.listdir(test_frames_dir)]\n",
    "test_Landmarks_files = [os.path.join(test_Landmarks_dir, filename) for filename in os.listdir(test_Landmarks_dir)]\n",
    "val_image_files = [os.path.join(val_frames_dir, filename) for filename in os.listdir(val_frames_dir)]\n",
    "val_Landmarks_files = [os.path.join(val_Landmarks_dir, filename) for filename in os.listdir(val_Landmarks_dir)]\n",
    "\n",
    "train_images = [load_image(file) for file in train_image_files]\n",
    "train_Landmarks = [load_landmarks(file) for file in train_Landmarks_files]\n",
    "\n",
    "test_images = [load_image(file) for file in test_image_files]\n",
    "test_Landmarks = [load_landmarks(file) for file in test_Landmarks_files]\n",
    "\n",
    "val_images = [load_image(file) for file in val_image_files]\n",
    "val_Landmarks = [load_landmarks(file) for file in val_Landmarks_files]\n",
    "\n",
    "# Data augmentation\n",
    "train_images_aug, train_Landmarks_aug = augment_data(train_images, train_Landmarks)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_Landmarks = np.array(train_Landmarks)\n",
    "test_images = np.array(test_images)\n",
    "test_Landmarks = np.array(test_Landmarks)\n",
    "val_images = np.array(val_images)\n",
    "val_Landmarks = np.array(val_Landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agremos el path de la carpeta de modelos para poder importar el modelo\n",
    "import sys\n",
    "sys.path.append(r'R:\\Codes\\Reto\\Modelos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion del modelo U-Net desde un archivo externo\n",
    "from model2 import unet_model\n",
    "\n",
    "# Crear el modelo U-Net\n",
    "model = unet_model(input_shape=(112, 112, 1), n_classes=7, kernel_out=1, activation='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mean_squared_error_landmarks(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss function for 7 landmarks.\n",
    "    Assumes y_true and y_pred are of shape (batch_size, 112, 112, 7).\n",
    "    \"\"\"\n",
    "    mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "    return mse\n",
    "\n",
    "def accuracy_landmarks(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Accuracy metric for 7 landmarks.\n",
    "    Assumes y_true and y_pred are of shape (batch_size, 112, 112, 7).\n",
    "    \"\"\"\n",
    "    # Extract the landmark values from y_true and y_pred\n",
    "    y_true_landmarks = y_true[..., :7]\n",
    "    y_pred_landmarks = y_pred[..., :7]\n",
    "\n",
    "    # Calculate the element-wise absolute difference\n",
    "    abs_diff = K.abs(y_true_landmarks - y_pred_landmarks)\n",
    "\n",
    "    # Create a mask where each element is 1 if the absolute difference is less than a threshold, otherwise 0\n",
    "    mask = K.cast(K.less(abs_diff, 0.5), dtype=tf.float32)\n",
    "\n",
    "    # Calculate the accuracy for each landmark\n",
    "    landmark_accuracy = K.mean(mask, axis=-1)\n",
    "\n",
    "    # Overall accuracy is the mean accuracy across all landmarks\n",
    "    overall_accuracy = K.mean(landmark_accuracy)\n",
    "\n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model history information to a json file\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a folder name\n",
    "name_folder = 'landmark_aug_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "path = '../Pruebas/{}/'.format(name_folder)\n",
    "os.mkdir(path)\n",
    "\n",
    "# Save history to a json file\n",
    "path_json = '{}history.json'.format(path)\n",
    "\n",
    "with open(path_json, 'w') as fp:\n",
    "    json.dump(history.history, fp)\n",
    "\n",
    "# Save model\n",
    "path_model = '{}model.h5'.format(path)\n",
    "model.save(path_model)\n",
    "\n",
    "# Save model summary to a txt file\n",
    "path_summary = '{}summary.txt'.format(path)\n",
    "with open(path_summary, 'w') as fp:\n",
    "    model.summary(print_fn=lambda x: fp.write(x + '\\n'))\n",
    "\n",
    "# Save model metrics to a txt file\n",
    "path_metrics = '{}metrics.txt'.format(path)\n",
    "with open(path_metrics, 'w') as fp:\n",
    "    fp.write('Test loss: {}\\n'.format(evaluation[0]))\n",
    "    fp.write('Test accuracy_landmarks: {}\\n'.format(evaluation[1]))\n",
    "    fp.write('Model optimizer: {}\\n'.format(model.optimizer.__class__.__name__))\n",
    "    fp.write('Model activation: {}\\n'.format(model.layers[-1].activation.__name__))\n",
    "    fp.write('Model learning rate: {}\\n'.format(model.optimizer.lr.numpy()))\n",
    "    fp.write('Model loss method: {}\\n'.format(loss_method))\n",
    "\n",
    "# Generate a folder inside the folder of the model for the images\n",
    "path_images = '{}images/'.format(path)\n",
    "os.mkdir(path_images)\n",
    "\n",
    "# Generate a folder inside the folder of the images for plot images\n",
    "path_plot_images = '{}plot_images/'.format(path_images)\n",
    "os.mkdir(path_plot_images)\n",
    "\n",
    "# Generate a folder inside the folder of the images for prediction images\n",
    "path_test_images = '{}predicitons/'.format(path_images)\n",
    "os.mkdir(path_test_images)\n",
    "\n",
    "# Save prediction images\n",
    "# Función para visualizar una muestra de imágenes y sus máscaras de segmentación predichas\n",
    "def visualize_landmarks(images, landmarks, predictions):\n",
    "    num_images = len(images)\n",
    "\n",
    "    for i in range(num_images):  # Renamed the inner loop variable to 'j'\n",
    "        # Set up the subplots\n",
    "        image = images[i].reshape((112, 112))\n",
    "        prediction = predictions[i].reshape((112, 112, 7)) * 255.0\n",
    "        landmark = landmarks[i] * 255.0  # No need to reshape\n",
    "        fig, axs = plt.subplots(2, 9, figsize=(15, 5))\n",
    "\n",
    "        # Display the test image spanning the first two columns of both rows\n",
    "        axs[0, 0].imshow(image, cmap='grey')\n",
    "        axs[0, 0].axis('off')\n",
    "        axs[0, 0].set_title('Test Image') \n",
    "        axs[1, 0].axis('off')\n",
    "\n",
    "        # Combine all channels of prediction_landmarks by taking the mean\n",
    "        combined_prediction = np.mean(prediction, axis=-1)\n",
    "\n",
    "        # CObine all channels of ground_truth_landmarks by taking the mean\n",
    "        combined_ground_truth = np.mean(landmark, axis=-1)\n",
    "\n",
    "        # Display the predicted and ground truth landmarks in the first column of the second row all in one image\n",
    "        axs[0, 1].imshow(combined_prediction, cmap='grey')\n",
    "        axs[0, 1].set_title('Predicted')\n",
    "        axs[0, 1].axis('off')\n",
    "        axs[1, 1].imshow(combined_ground_truth, cmap='grey')\n",
    "        axs[1, 1].set_title('Ground Truth')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "        # Display the predicted landmarks in the first row\n",
    "        for k in range(7):  # Renamed the loop variable to 'k'\n",
    "            axs[0, k+2].imshow(prediction[ :, :, k], cmap='grey')\n",
    "            axs[0, k+2].axis('off')\n",
    "            axs[0, k+2].set_title('Predicted ' + str(k + 1))\n",
    "\n",
    "        # Display the ground truth landmarks in the second row\n",
    "        for k in range(7):  # Renamed the loop variable to 'k'\n",
    "            axs[1, k+2].imshow(landmark[ :, :, k], cmap='grey')\n",
    "            axs[1, k+2].axis('off')\n",
    "            axs[1, k+2].set_title('Ground Truth ' + str(k + 1))\n",
    "\n",
    "        plt.savefig('{}{}.png'.format(path_test_images, i))\n",
    "\n",
    "\n",
    "# Supongamos que tienes un conjunto de datos de prueba con imágenes y máscaras\n",
    "# Puedes utilizar el modelo para obtener las máscaras predichas en el conjunto de prueba\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Elije algunas muestras aleatorias para visualizar\n",
    "num_samples_to_visualize = 5\n",
    "sample_indices = np.random.choice(len(test_images[:10]), num_samples_to_visualize, replace=False)\n",
    "\n",
    "sample_images = [test_images[i] for i in sample_indices]\n",
    "sample_landmarks = [test_Landmarks[i] for i in sample_indices]\n",
    "sample_predictions = [predictions[i] for i in sample_indices]\n",
    "\n",
    "# Llama a la función para visualizar las imágenes y máscaras\n",
    "visualize_landmarks(sample_images, sample_landmarks, sample_predictions)\n",
    "\n",
    "# Save plot images\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy_landmarks'])\n",
    "plt.plot(history.history['val_accuracy_landmarks'])\n",
    "plt.title('Model accuracy_landmarks')\n",
    "plt.ylabel('accuracy_landmarks')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('{}{}.png'.format(path_plot_images, 'accuracy_landmarks'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('{}loss.png'.format(path_plot_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a txt file with the augmentation information\n",
    "path_aug = '{}aug.txt'.format(path)\n",
    "\n",
    "with open(path_aug, 'w') as fp:\n",
    "    # Write the technique used\n",
    "    fp.write('Data augmentation: Flip Left Right\\n')\n",
    "    # fp.write('Data augmentation: Rotations 20 degrees\\n')\n",
    "    # fp.write('Data augmentation: Rotations -20 degrees\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "#TODO: Investigate what the ssim function does\n",
    "def calculate_ssim(image1, image2):\n",
    "    # Calculate the Structural Similarity Index (SSI)\n",
    "    similarity_index, _ = ssim(image1, image2, full=True, data_range=image1.max() - image1.min())\n",
    "\n",
    "    # The SSI ranges from -1 to 1, where 1 indicates a perfect match\n",
    "    # We normalize it to the range [0, 1] to represent percentage similarity\n",
    "    percentage_similarity = (similarity_index + 1) / 2 * 100\n",
    "\n",
    "    return percentage_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the mean Structural Similarity Index (SSI) for the landmarks in the test set\n",
    "mean_ssim_landmark = 0\n",
    "for i in range(len(test_Landmarks)):\n",
    "    landmark1 = np.mean(test_Landmarks[i], axis=-1)\n",
    "    landmark2 = np.mean(predictions[i], axis=-1)\n",
    "    mean_ssim_landmark += calculate_ssim(landmark1, landmark2)\n",
    "\n",
    "mean_ssim_landmark /= len(test_Landmarks)\n",
    "\n",
    "# Calculate the mean absolute error for the landmarks in the test set\n",
    "mean_abs_error_landmark = 0\n",
    "for i in range(len(test_Landmarks)):\n",
    "    landmark1 = np.mean(test_Landmarks[i], axis=-1)\n",
    "    landmark2 = np.mean(predictions[i], axis=-1)\n",
    "    mean_abs_error_landmark += np.mean(np.abs(landmark1 - landmark2))\n",
    "\n",
    "mean_abs_error_landmark /= len(test_Landmarks)\n",
    "\n",
    "# Save the mean SSI and mean absolute error to a file\n",
    "f = open(path + \"/results.txt\", \"a\")\n",
    "f.write(\"\\n\")\n",
    "f.write(\"Mean SSI for Landmarks: \" + str(mean_ssim_landmark) + \"\\n\")\n",
    "f.write(\"Mean absolute error for Landmarks: \" + str(mean_abs_error_landmark) + \"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
