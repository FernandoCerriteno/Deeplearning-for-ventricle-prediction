{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Allow GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Initialize GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.transform import rotate\n",
    "\n",
    "# Rutas a las carpetas de entrenamiento, prueba y validación para imágenes y máscaras\n",
    "train_frames_dir = '../Frames/TRAIN'\n",
    "train_Landmarks_dir = './Landmarks/TRAIN'\n",
    "test_frames_dir = '../Frames/TEST'\n",
    "test_Landmarks_dir = './Landmarks/TEST'\n",
    "val_frames_dir = '../Frames/VAL'\n",
    "val_Landmarks_dir = './Landmarks/VAL'\n",
    "\n",
    "# Funciones para cargar y preprocesar imagen y máscara y convertirlo a solo un canal\n",
    "def load_image(file_path):\n",
    "    image = cv2.imread(file_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def load_landmarks(file_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(file_path):\n",
    "        image=cv2.imread(os.path.join(file_path, filename))\n",
    "        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        images.append(grayscale_image)\n",
    "\n",
    "    stacked_image = np.stack(images, axis=-1)  # axis=-1 indica la dimensión de los canales\n",
    "    stacked_image = stacked_image / 255.0\n",
    "    return stacked_image\n",
    "\n",
    "# Data augmentation: Rotations and Flips\n",
    "def augment_data(images, landmarks):\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "    for image, mask in zip(images, landmarks):\n",
    "        augmented_images.append(image)\n",
    "        augmented_masks.append(landmarks)\n",
    "        # # 20 degree rotations\n",
    "        # augmented_images.append(rotate(image, angle=20, reshape=False))\n",
    "        # augmented_masks.append(rotate(landmarks, angle=20, reshape=False))\n",
    "        # augmented_images.append(rotate(image, angle=-20, reshape=False))\n",
    "        # augmented_masks.append(rotate(landmarks, angle=-20, reshape=False))\n",
    "\n",
    "        # Flips\n",
    "        augmented_images.append(np.fliplr(image))\n",
    "        augmented_masks.append(np.fliplr(landmarks))\n",
    "\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "train_image_files = [os.path.join(train_frames_dir, filename) for filename in os.listdir(train_frames_dir)]\n",
    "train_Landmarks_files = [os.path.join(train_Landmarks_dir, filename) for filename in os.listdir(train_Landmarks_dir)]\n",
    "test_image_files = [os.path.join(test_frames_dir, filename) for filename in os.listdir(test_frames_dir)]\n",
    "test_Landmarks_files = [os.path.join(test_Landmarks_dir, filename) for filename in os.listdir(test_Landmarks_dir)]\n",
    "val_image_files = [os.path.join(val_frames_dir, filename) for filename in os.listdir(val_frames_dir)]\n",
    "val_Landmarks_files = [os.path.join(val_Landmarks_dir, filename) for filename in os.listdir(val_Landmarks_dir)]\n",
    "\n",
    "test_images = [load_image(file) for file in test_image_files]\n",
    "test_Landmarks = [load_landmarks(file) for file in test_Landmarks_files]\n",
    "\n",
    "val_images = [load_image(file) for file in val_image_files]\n",
    "val_Landmarks = [load_landmarks(file) for file in val_Landmarks_files]\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_Landmarks = np.array(test_Landmarks)\n",
    "val_images = np.array(val_images)\n",
    "val_Landmarks = np.array(val_Landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agremos el path de la carpeta de modelos para poder importar el modelo\n",
    "import sys\n",
    "sys.path.append(r'R:\\Codes\\Reto\\Modelos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion del modelo U-Net desde un archivo externo\n",
    "from model2 import unet_model\n",
    "\n",
    "# Crear el modelo U-Net\n",
    "model = unet_model(input_shape=(112, 112, 1), n_classes=7, kernel_out=1, activation='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mean_squared_error_landmarks(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss function for 7 landmarks.\n",
    "    Assumes y_true and y_pred are of shape (batch_size, 112, 112, 7).\n",
    "    \"\"\"\n",
    "    mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "    return mse\n",
    "\n",
    "def accuracy_landmarks(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Accuracy metric for 7 landmarks.\n",
    "    Assumes y_true and y_pred are of shape (batch_size, 112, 112, 7).\n",
    "    \"\"\"\n",
    "    # Extract the landmark values from y_true and y_pred\n",
    "    y_true_landmarks = y_true[..., :7]\n",
    "    y_pred_landmarks = y_pred[..., :7]\n",
    "\n",
    "    # Calculate the element-wise absolute difference\n",
    "    abs_diff = K.abs(y_true_landmarks - y_pred_landmarks)\n",
    "\n",
    "    # Create a mask where each element is 1 if the absolute difference is less than a threshold, otherwise 0\n",
    "    mask = K.cast(K.less(abs_diff, 0.5), dtype=tf.float32)\n",
    "\n",
    "    # Calculate the accuracy for each landmark\n",
    "    landmark_accuracy = K.mean(mask, axis=-1)\n",
    "\n",
    "    # Overall accuracy is the mean accuracy across all landmarks\n",
    "    overall_accuracy = K.mean(landmark_accuracy)\n",
    "\n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'U-Net_2/batch_normalization_89/FusedBatchNormV3' defined at (most recent call last):\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n      app.start()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\luisr\\AppData\\Local\\Temp\\ipykernel_12344\\761714961.py\", line 42, in <module>\n      history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=callbacks)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 850, in call\n      outputs = self._fused_batch_norm(inputs, training=training)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 660, in _fused_batch_norm\n      output, mean, variance = control_flow_util.smart_cond(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 634, in _fused_batch_norm_training\n      return tf.compat.v1.nn.fused_batch_norm(\nNode: 'U-Net_2/batch_normalization_89/FusedBatchNormV3'\nOOM when allocating tensor with shape[8,64,112,112] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node U-Net_2/batch_normalization_89/FusedBatchNormV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_570124]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mr:\\Codes\\Reto\\Landmarks\\modelo_unet_landmarks_aug.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/r%3A/Codes/Reto/Landmarks/modelo_unet_landmarks_aug.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mbatch(BATCH_SIZE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/r%3A/Codes/Reto/Landmarks/modelo_unet_landmarks_aug.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Train the model for one epoch\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/r%3A/Codes/Reto/Landmarks/modelo_unet_landmarks_aug.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mval_dataset, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[1;32mc:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'U-Net_2/batch_normalization_89/FusedBatchNormV3' defined at (most recent call last):\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n      app.start()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\luisr\\AppData\\Local\\Temp\\ipykernel_12344\\761714961.py\", line 42, in <module>\n      history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=callbacks)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 850, in call\n      outputs = self._fused_batch_norm(inputs, training=training)\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 660, in _fused_batch_norm\n      output, mean, variance = control_flow_util.smart_cond(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"c:\\Users\\luisr\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 634, in _fused_batch_norm_training\n      return tf.compat.v1.nn.fused_batch_norm(\nNode: 'U-Net_2/batch_normalization_89/FusedBatchNormV3'\nOOM when allocating tensor with shape[8,64,112,112] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node U-Net_2/batch_normalization_89/FusedBatchNormV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_570124]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Early stopping and model checkpoint callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10, mode='min', monitor='val_loss'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "with tf.device('CPU'):\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_Landmarks))\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_Landmarks))\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# Compile the model outside the loop\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error', accuracy_landmarks])\n",
    "\n",
    "for i in range(0, len(train_image_files), BATCH_SIZE):\n",
    "    batch_image_paths = train_image_files[i:i+BATCH_SIZE]\n",
    "    batch_landmark_paths = train_Landmarks_files[i:i+BATCH_SIZE]\n",
    "\n",
    "    # Load and pre-process the images and landmarks\n",
    "    batch_images = [load_image(file) for file in batch_image_paths]\n",
    "    batch_landmarks = [load_landmarks(file) for file in batch_landmark_paths]\n",
    "\n",
    "    # Data augmentation\n",
    "    batch_images_aug, batch_landmarks_aug = augment_data(batch_images, batch_landmarks)\n",
    "\n",
    "    # Convert the list of images and landmarks to a numpy array\n",
    "    batch_images_aug = np.array(batch_images_aug)\n",
    "    batch_landmarks_aug = np.array(batch_landmarks_aug)\n",
    "\n",
    "    with tf.device('CPU'):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((batch_images_aug, batch_landmarks_aug))\n",
    "        train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model history information to a json file\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a folder name\n",
    "name_folder = 'landmark_aug_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "path = '../Pruebas/{}/'.format(name_folder)\n",
    "os.mkdir(path)\n",
    "\n",
    "# Save history to a json file\n",
    "path_json = '{}history.json'.format(path)\n",
    "\n",
    "with open(path_json, 'w') as fp:\n",
    "    json.dump(history.history, fp)\n",
    "\n",
    "# Save model\n",
    "path_model = '{}model.h5'.format(path)\n",
    "model.save(path_model)\n",
    "\n",
    "# Save model summary to a txt file\n",
    "path_summary = '{}summary.txt'.format(path)\n",
    "with open(path_summary, 'w') as fp:\n",
    "    model.summary(print_fn=lambda x: fp.write(x + '\\n'))\n",
    "\n",
    "# Save model metrics to a txt file\n",
    "path_metrics = '{}metrics.txt'.format(path)\n",
    "with open(path_metrics, 'w') as fp:\n",
    "    fp.write('Test loss: {}\\n'.format(evaluation[0]))\n",
    "    fp.write('Test accuracy_landmarks: {}\\n'.format(evaluation[1]))\n",
    "    fp.write('Model optimizer: {}\\n'.format(model.optimizer.__class__.__name__))\n",
    "    fp.write('Model activation: {}\\n'.format(model.layers[-1].activation.__name__))\n",
    "    fp.write('Model learning rate: {}\\n'.format(model.optimizer.lr.numpy()))\n",
    "    fp.write('Model loss method: {}\\n'.format(loss_method))\n",
    "\n",
    "# Generate a folder inside the folder of the model for the images\n",
    "path_images = '{}images/'.format(path)\n",
    "os.mkdir(path_images)\n",
    "\n",
    "# Generate a folder inside the folder of the images for plot images\n",
    "path_plot_images = '{}plot_images/'.format(path_images)\n",
    "os.mkdir(path_plot_images)\n",
    "\n",
    "# Generate a folder inside the folder of the images for prediction images\n",
    "path_test_images = '{}predicitons/'.format(path_images)\n",
    "os.mkdir(path_test_images)\n",
    "\n",
    "# Save prediction images\n",
    "# Función para visualizar una muestra de imágenes y sus máscaras de segmentación predichas\n",
    "def visualize_landmarks(images, landmarks, predictions):\n",
    "    num_images = len(images)\n",
    "\n",
    "    for i in range(num_images):  # Renamed the inner loop variable to 'j'\n",
    "        # Set up the subplots\n",
    "        image = images[i].reshape((112, 112))\n",
    "        prediction = predictions[i].reshape((112, 112, 7)) * 255.0\n",
    "        landmark = landmarks[i] * 255.0  # No need to reshape\n",
    "        fig, axs = plt.subplots(2, 9, figsize=(15, 5))\n",
    "\n",
    "        # Display the test image spanning the first two columns of both rows\n",
    "        axs[0, 0].imshow(image, cmap='grey')\n",
    "        axs[0, 0].axis('off')\n",
    "        axs[0, 0].set_title('Test Image') \n",
    "        axs[1, 0].axis('off')\n",
    "\n",
    "        # Combine all channels of prediction_landmarks by taking the mean\n",
    "        combined_prediction = np.mean(prediction, axis=-1)\n",
    "\n",
    "        # CObine all channels of ground_truth_landmarks by taking the mean\n",
    "        combined_ground_truth = np.mean(landmark, axis=-1)\n",
    "\n",
    "        # Display the predicted and ground truth landmarks in the first column of the second row all in one image\n",
    "        axs[0, 1].imshow(combined_prediction, cmap='grey')\n",
    "        axs[0, 1].set_title('Predicted')\n",
    "        axs[0, 1].axis('off')\n",
    "        axs[1, 1].imshow(combined_ground_truth, cmap='grey')\n",
    "        axs[1, 1].set_title('Ground Truth')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "        # Display the predicted landmarks in the first row\n",
    "        for k in range(7):  # Renamed the loop variable to 'k'\n",
    "            axs[0, k+2].imshow(prediction[ :, :, k], cmap='grey')\n",
    "            axs[0, k+2].axis('off')\n",
    "            axs[0, k+2].set_title('Predicted ' + str(k + 1))\n",
    "\n",
    "        # Display the ground truth landmarks in the second row\n",
    "        for k in range(7):  # Renamed the loop variable to 'k'\n",
    "            axs[1, k+2].imshow(landmark[ :, :, k], cmap='grey')\n",
    "            axs[1, k+2].axis('off')\n",
    "            axs[1, k+2].set_title('Ground Truth ' + str(k + 1))\n",
    "\n",
    "        plt.savefig('{}{}.png'.format(path_test_images, i))\n",
    "\n",
    "\n",
    "# Supongamos que tienes un conjunto de datos de prueba con imágenes y máscaras\n",
    "# Puedes utilizar el modelo para obtener las máscaras predichas en el conjunto de prueba\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Elije algunas muestras aleatorias para visualizar\n",
    "num_samples_to_visualize = 5\n",
    "sample_indices = np.random.choice(len(test_images[:10]), num_samples_to_visualize, replace=False)\n",
    "\n",
    "sample_images = [test_images[i] for i in sample_indices]\n",
    "sample_landmarks = [test_Landmarks[i] for i in sample_indices]\n",
    "sample_predictions = [predictions[i] for i in sample_indices]\n",
    "\n",
    "# Llama a la función para visualizar las imágenes y máscaras\n",
    "visualize_landmarks(sample_images, sample_landmarks, sample_predictions)\n",
    "\n",
    "# Save plot images\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy_landmarks'])\n",
    "plt.plot(history.history['val_accuracy_landmarks'])\n",
    "plt.title('Model accuracy_landmarks')\n",
    "plt.ylabel('accuracy_landmarks')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('{}{}.png'.format(path_plot_images, 'accuracy_landmarks'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('{}loss.png'.format(path_plot_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a txt file with the augmentation information\n",
    "path_aug = '{}aug.txt'.format(path)\n",
    "\n",
    "with open(path_aug, 'w') as fp:\n",
    "    # Write the technique used\n",
    "    fp.write('Data augmentation: Flip Left Right\\n')\n",
    "    # fp.write('Data augmentation: Rotations 20 degrees\\n')\n",
    "    # fp.write('Data augmentation: Rotations -20 degrees\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "#TODO: Investigate what the ssim function does\n",
    "def calculate_ssim(image1, image2):\n",
    "    # Calculate the Structural Similarity Index (SSI)\n",
    "    similarity_index, _ = ssim(image1, image2, full=True, data_range=image1.max() - image1.min())\n",
    "\n",
    "    # The SSI ranges from -1 to 1, where 1 indicates a perfect match\n",
    "    # We normalize it to the range [0, 1] to represent percentage similarity\n",
    "    percentage_similarity = (similarity_index + 1) / 2 * 100\n",
    "\n",
    "    return percentage_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the mean Structural Similarity Index (SSI) for the landmarks in the test set\n",
    "mean_ssim_landmark = 0\n",
    "for i in range(len(test_Landmarks)):\n",
    "    landmark1 = np.mean(test_Landmarks[i], axis=-1)\n",
    "    landmark2 = np.mean(predictions[i], axis=-1)\n",
    "    mean_ssim_landmark += calculate_ssim(landmark1, landmark2)\n",
    "\n",
    "mean_ssim_landmark /= len(test_Landmarks)\n",
    "\n",
    "# Calculate the mean absolute error for the landmarks in the test set\n",
    "mean_abs_error_landmark = 0\n",
    "for i in range(len(test_Landmarks)):\n",
    "    landmark1 = np.mean(test_Landmarks[i], axis=-1)\n",
    "    landmark2 = np.mean(predictions[i], axis=-1)\n",
    "    mean_abs_error_landmark += np.mean(np.abs(landmark1 - landmark2))\n",
    "\n",
    "mean_abs_error_landmark /= len(test_Landmarks)\n",
    "\n",
    "# Save the mean SSI and mean absolute error to a file\n",
    "f = open(path + \"/results.txt\", \"a\")\n",
    "f.write(\"\\n\")\n",
    "f.write(\"Mean SSI for Landmarks: \" + str(mean_ssim_landmark) + \"\\n\")\n",
    "f.write(\"Mean absolute error for Landmarks: \" + str(mean_abs_error_landmark) + \"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
